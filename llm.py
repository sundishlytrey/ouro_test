# ============================================
# –ü–û–õ–ù–´–ô –ó–ê–ü–£–°–ö OUROBOROS –° –ë–ï–°–ü–õ–ê–¢–ù–´–ú–ò –ú–û–î–ï–õ–Ø–ú–ò (–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø)
# ============================================

# 1. –£–°–¢–ê–ù–ê–í–õ–ò–í–ê–ï–ú –í–°–Å –ß–¢–û –ù–£–ñ–ù–û
!pip install -q requests python-telegram-bot python-dotenv

# 2. –ö–õ–û–ù–ò–†–£–ï–ú –†–ï–ü–û–ó–ò–¢–û–†–ò–ô
!git clone https://github.com/razzant/ouroboros.git /content/ouroboros_repo
%cd /content/ouroboros_repo

# 3. –ü–û–õ–ù–û–°–¢–¨–Æ –ó–ê–ú–ï–ù–Ø–ï–ú –§–ê–ô–õ LLM.PY –ù–ê –ë–ï–°–ü–õ–ê–¢–ù–£–Æ –í–ï–†–°–ò–Æ (–° –î–û–ë–ê–í–õ–ï–ù–ù–´–ú–ò –ö–û–ù–°–¢–ê–ù–¢–ê–ú–ò)
with open('/content/ouroboros_repo/ouroboros/llm.py', 'w') as f:
    f.write('''"""
LLM client for GitHub Models (Mistral, DeepSeek, Phi, Llama).
Supports multiple free models with fallback.
For Russia: no credits, no OpenRouter, just a GitHub token.
"""
import os
import json
import time
import requests
from typing import Optional, Dict, Any, List, Union

# GitHub Models inference endpoint
GITHUB_MODELS_ENDPOINT = "https://models.inference.ai.azure.com"

# Available free models on GitHub
MODEL_LIST = {
    "mistralai/Mistral-7B-Instruct-v0.3": "mistral-7b",
    "mistralai/Mistral-Nemo-Instruct-2407": "mistral-nemo",
    "mistralai/Mixtral-8x7B-Instruct-v0.1": "mixtral",
    "deepseek-ai/DeepSeek-R1": "deepseek-r1",
    "deepseek-ai/DeepSeek-V3": "deepseek-v3",
    "microsoft/Phi-3.5-mini-instruct": "phi-3.5-mini",
    "microsoft/Phi-3.5-MoE-instruct": "phi-3.5-moe",
    "microsoft/Phi-3.5-vision-instruct": "phi-3.5-vision",
    "microsoft/Phi-4": "phi-4",
    "meta-llama/Llama-3.2-11B-Vision-Instruct": "llama-3.2-11b",
    "meta-llama/Llama-3.2-90B-Vision-Instruct": "llama-3.2-90b",
    "meta-llama/Llama-3.3-70B-Instruct": "llama-3.3-70b",
    "meta-llama/Llama-Guard-3-11B-Vision": "llama-guard",
    "ai21-ai/Jamba-Instruct": "jamba-instruct",
    "cohere-ai/Command-R": "command-r",
    "cohere-ai/Command-R-Plus": "command-r-plus",
    "nomic-ai/Nomic-Embed-Text-v1.5": "nomic-embed",
}

MODEL_NAME_TO_ID = {v: k for k, v in MODEL_LIST.items()}

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω—ã –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º –∫–æ–¥–æ–º
DEFAULT_MODEL = "mistral-nemo"
DEFAULT_LIGHT_MODEL = "phi-3.5-mini"
DEFAULT_CODE_MODEL = "deepseek-r1"

class LLMClient:
    """Client for GitHub Models API (free, token-based)"""
    
    def __init__(self, model: str = "mistralai/Mistral-Nemo-Instruct-2407"):
        if model in MODEL_NAME_TO_ID:
            self.model = MODEL_NAME_TO_ID[model]
        elif model in MODEL_LIST:
            self.model = model
        else:
            print(f"‚ö†Ô∏è Unknown model '{model}', defaulting to Mistral Nemo")
            self.model = "mistralai/Mistral-Nemo-Instruct-2407"
        
        self.token = os.environ.get("GITHUB_TOKEN")
        if not self.token:
            raise ValueError("‚ùå GITHUB_TOKEN not found in environment")
        
        self.headers = {
            "Authorization": f"Bearer {self.token}",
            "Content-Type": "application/json"
        }
        
        self.last_usage = {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0,
            "cost_usd": 0.0
        }
        
        print(f"‚úÖ LLM Client initialized with model: {self.model}")
        print(f"üí∞ Using GitHub Models - 100% FREE!")
    
    def _prepare_messages(self, prompt: Union[str, List[Dict[str, str]]], system: Optional[str] = None):
        messages = []
        if system:
            messages.append({"role": "system", "content": system})
        if isinstance(prompt, str):
            messages.append({"role": "user", "content": prompt})
        elif isinstance(prompt, list):
            messages.extend(prompt)
        else:
            raise ValueError(f"Unsupported prompt type: {type(prompt)}")
        return messages
    
    def generate(
        self,
        prompt: Union[str, List[Dict[str, str]]],
        system: Optional[str] = None,
        max_tokens: int = 4000,
        temperature: float = 0.7,
        stop: Optional[List[str]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        messages = self._prepare_messages(prompt, system)
        
        body = {
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "stream": False
        }
        if stop:
            body["stop"] = stop
        
        url = f"{GITHUB_MODELS_ENDPOINT}/chat/completions"
        
        try:
            response = requests.post(
                url,
                headers=self.headers,
                json=body,
                timeout=120
            )
            response.raise_for_status()
            result = response.json()
            
            content = result["choices"][0]["message"]["content"]
            
            usage = result.get("usage", {})
            prompt_tokens = usage.get("prompt_tokens", 0)
            completion_tokens = usage.get("completion_tokens", 0)
            
            self.last_usage = {
                "prompt_tokens": prompt_tokens,
                "completion_tokens": completion_tokens,
                "total_tokens": prompt_tokens + completion_tokens,
                "cost_usd": 0.0
            }
            
            return {
                "content": content.strip(),
                "usage": self.last_usage,
                "model": self.model,
                "finish_reason": result["choices"][0].get("finish_reason", "stop")
            }
            
        except requests.exceptions.RequestException as e:
            error_msg = f"GitHub Models API error: {str(e)}"
            return {
                "content": "",
                "error": error_msg,
                "usage": self.last_usage,
                "model": self.model
            }
    
    def count_tokens(self, text: str) -> int:
        return len(text) // 4
    
    def get_usage(self) -> Dict[str, Any]:
        return self.last_usage


class MultiLLMClient:
    """Client that tries multiple models in sequence."""
    
    def __init__(self, models: List[str], fallback_to_any: bool = True):
        self.models = models
        self.fallback_to_any = fallback_to_any
        self.current_client = None
        self.last_error = None
        
    def generate(self, *args, **kwargs):
        errors = []
        
        for model_name in self.models:
            try:
                print(f"üîÑ Trying model: {model_name}")
                client = LLMClient(model=model_name)
                result = client.generate(*args, **kwargs)
                
                if result.get("content") and not result.get("error"):
                    self.current_client = client
                    return result
                else:
                    error = result.get("error", "Empty response")
                    errors.append(f"{model_name}: {error}")
                    
            except Exception as e:
                errors.append(f"{model_name}: {str(e)}")
                continue
        
        if self.fallback_to_any:
            print("‚ö†Ô∏è Specified models failed, trying any available model...")
            
            tried_models = set(self.models)
            for full_model in MODEL_LIST.keys():
                for short, full in MODEL_NAME_TO_ID.items():
                    if full == full_model and short not in tried_models:
                        try:
                            print(f"üîÑ Fallback trying: {short}")
                            client = LLMClient(model=short)
                            result = client.generate(*args, **kwargs)
                            
                            if result.get("content") and not result.get("error"):
                                self.current_client = client
                                return result
                            else:
                                error = result.get("error", "Empty response")
                                errors.append(f"{short}: {error}")
                        except Exception as e:
                            errors.append(f"{short}: {str(e)}")
                        break
        
        error_summary = "\\n".join(errors[-5:])
        return {
            "content": "",
            "error": f"All models failed. Last errors:\\n{error_summary}",
            "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0, "cost_usd": 0.0},
            "model": "none"
        }


def complete(prompt: str, model: Optional[str] = None, **kwargs) -> str:
    """Simple completion function for backward compatibility."""
    if model is None:
        model = os.environ.get("OUROBOROS_MODEL", "mistral-nemo")
    
    client = LLMClient(model=model)
    result = client.generate(prompt, **kwargs)
    
    if result.get("error"):
        print(f"‚ö†Ô∏è Completion error: {result['error']}")
        return ""
    
    return result.get("content", "")
''')

print("‚úÖ –§–∞–π–ª llm.py —É—Å–ø–µ—à–Ω–æ –∑–∞–º–µ–Ω—ë–Ω –Ω–∞ –±–µ—Å–ø–ª–∞—Ç–Ω—É—é –≤–µ—Ä—Å–∏—é (—Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞–º–∏)!")

# 4. –ü–û–õ–£–ß–ê–ï–ú –¢–û–ö–ï–ù–´ –ò–ó –°–ï–ö–†–ï–¢–û–í COLAB
from google.colab import userdata
import os

try:
    # –ó–∞–±–∏—Ä–∞–µ–º —Ç–æ–∫–µ–Ω—ã –∏–∑ —Å–µ–∫—Ä–µ—Ç–æ–≤ Colab
    GITHUB_TOKEN = userdata.get('GITHUB_TOKEN')
    TELEGRAM_BOT_TOKEN = userdata.get('TELEGRAM_BOT_TOKEN')
    
    if not GITHUB_TOKEN or not TELEGRAM_BOT_TOKEN:
        raise ValueError("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã —Ç–æ–∫–µ–Ω—ã –≤ —Å–µ–∫—Ä–µ—Ç–∞—Ö Colab!")
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
    os.environ["GITHUB_TOKEN"] = GITHUB_TOKEN
    os.environ["TELEGRAM_BOT_TOKEN"] = TELEGRAM_BOT_TOKEN
    
    print("‚úÖ –¢–æ–∫–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ —Å–µ–∫—Ä–µ—Ç–æ–≤ Colab")
    
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤: {e}")
    print("\nüëâ –ò–ù–°–¢–†–£–ö–¶–ò–Ø:")
    print("1. –ù–∞–∂–º–∏ –Ω–∞ –∑–Ω–∞—á–æ–∫ üîë (Secrets) –≤ –ª–µ–≤–æ–π –ø–∞–Ω–µ–ª–∏ Colab")
    print("2. –î–æ–±–∞–≤—å –¥–≤–∞ —Å–µ–∫—Ä–µ—Ç–∞:")
    print("   - –ò–º—è: GITHUB_TOKEN    –ó–Ω–∞—á–µ–Ω–∏–µ: —Ç–≤–æ–π GitHub —Ç–æ–∫–µ–Ω")
    print("   - –ò–º—è: TELEGRAM_BOT_TOKEN    –ó–Ω–∞—á–µ–Ω–∏–µ: —Ç–æ–∫–µ–Ω —Ç–≤–æ–µ–≥–æ –±–æ—Ç–∞")
    print("3. –î–ª—è –æ–±–æ–∏—Ö –≤–∫–ª—é—á–∏ 'Notebook access'")
    print("4. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏ —ç—Ç—É —è—á–µ–π–∫—É")
    raise

# 5. –¢–í–û–ò –ù–ê–°–¢–†–û–ô–ö–ò (–ó–ê–ú–ï–ù–ò –≠–¢–û!)
GITHUB_USERNAME = "sundishlytrey"  # <--- –í–°–¢–ê–í–¨ –°–í–û–Å –ò–ú–Ø –° –ì–ò–¢–•–ê–ë–ê

# 6. –ù–ê–°–¢–†–û–ô–ö–ò –ú–û–î–ï–õ–ï–ô (–í–°–Å –ë–ï–°–ü–õ–ê–¢–ù–û!)
os.environ["OUROBOROS_MODEL"] = "mistral-nemo"        # –û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å
os.environ["OUROBOROS_MODEL_CODE"] = "deepseek-r1"    # –î–ª—è –∫–æ–¥–∞
os.environ["OUROBOROS_MODEL_LIGHT"] = "phi-3.5-mini"  # –î–ª—è —Ñ–æ–Ω–∞
os.environ["OUROBOROS_MODEL_FALLBACK_LIST"] = "mistral-nemo,deepseek-r1,phi-3.5-mini,llama-3.2-11b"

# –ë—é–¥–∂–µ—Ç (–ø—Ä–æ—Å—Ç–æ –∑–∞–≥–ª—É—à–∫–∞, –¥–µ–Ω—å–≥–∏ –Ω–µ —Ç—Ä–∞—Ç—è—Ç—Å—è)
os.environ["TOTAL_BUDGET"] = "100"

print("\nüöÄ –í–°–Å –ì–û–¢–û–í–û! –ó–∞–ø—É—Å–∫–∞–µ–º –∞–≥–µ–Ω—Ç–∞...\n")

# 7. –°–û–ó–î–ê–Å–ú –§–ê–ô–õ –° –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ï–ô –î–õ–Ø –ó–ê–ü–£–°–ö–ê
with open('/content/ouroboros_repo/run_config.py', 'w') as f:
    f.write(f'''
import os
os.environ["GITHUB_USER"] = "sundishlytrey"
os.environ["GITHUB_REPO"] = "ouro_test"
''')

# 8. –ó–ê–ü–£–°–ö–ê–ï–ú –ê–ì–ï–ù–¢–ê (–æ–±–Ω–æ–≤–ª—ë–Ω–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞)
!cd /content/ouroboros_repo && python colab_launcher.py --github_user={GITHUB_USERNAME}
